What is prompt engineering in the context of Large Language Models (LLMs)?
A: Iteratively refining the ask to elicit a desired response

What does the term "hallucination" refer to in the context of Language Large Models (LLMs)?
The phenomenon where the model generates factually incorrect information or unrelated content as if it were true

What is the role of temperature in the decoding process of a Large Language Model (LLM)?
To adjust the sharpness of probability distribution over vocabulary when selecting the next word

Fine-tuning modifies all parameters using labeled, task-specific data, whereas Parameter Efficient Fine-Tuning updates a few, new parameters also with labeled, task-specific data.

What does in-context learning in Large Language Models involve?
Conditioning the model with task-specific instructions or demonstrations

Temperature:
For model to be deterministic : 0 - selects blue (the word with the highest probability)
For model to be creative: > 0  - may select any other words
When temperature is increased, the distribution is flattened over all words

The sky is blue ---
blue 0.45,  the limit 0.25, red 0.3, water 0.01, tarnish 0.02

Top k : selects top k tokens in sorted order of probabilities.
Top p: selects top tokens based on the sum of their probabilities.
       Top p = .75 means, exclude the bottom .25 tokens.

Stop sequence: String that tells model to stop generating output content.
               If a period (.) is used as stop sequence, the model stops generating output at the end of the first sentence even if the no.of tokens limit is much higher.

Frequency and Presence penalties: To get output to be less repetitive text.

Show Likelihood: Every time a new token is generated, a number between -15 and 0 is assigned to all tokens.

Types of models: Generation Models, Summarization Models, Embedding Models
Model does: Text Generation, Text Extraction, Text Classification.

Embeddings are numerical representations of a piece of text converted to number sequences.

In-context learning: prompting an LLM with instructions and or demonstrations of the task it is meant to complete.
k-shot prompting (few-shot prompting): explicitly providing k examples of the intended task in the prompt.

Prompting strategies:
chain-of-thought: provide examples. includes a reasoning step.

Zero-shot chain-of-thought: apply chain-of-thought prompting without providing examples.

Prompt Engineering vs Fine-tuning vs RAG

Vector Embeddings:
It transforms linguistic meaning into geometric relationships that can be measured and analyzed mathematically.



. . .

References
Unicode and Byte Pair Encoding (BPE)
===============================
https://www.reedbeta.com/blog/programmers-intro-to-unicode/

Matrix Math
===========
https://machinelearningmastery.com/the-transformer-attention-mechanism/
https://machinelearningmastery.com/the-attention-mechanism-from-scratch/

Embedding Spaces
================
https://lakshyamalhotra.github.io/2024/06/10/Mathematical-details-behind-self-attention.html
https://www.lesswrong.com/posts/pHPmMGEMYefk9jLeh/llm-basics-embedding-spaces-transformer-token-vectors-are

Transformers
============
0. https://www.youtube.com/watch?v=KCXDr-UOb9A
    https://github.com/srush/LLM-Talk/blob/main/Tutorial.pdf
    https://www.youtube.com/watch?v=k9DnQPrfJQs (Sasha Rush)

1. https://github.com/hkproj/transformer-from-scratch-notes
    https://www.youtube.com/watch?v=bCz4OMemCcA

2. Videos explaining Math behind Q, K, and V
    https://www.youtube.com/@machinelearningcourses4764 (Machine Learning Courses)
      https://www.youtube.com/watch?v=0XH0B8uMPKA&t=11s
    https://www.youtube.com/watch?v=OxCpWwDCDFQ (Serrano.Academy)
    https://www.youtube.com/watch?v=H-4bmOxiKyU (Q, K, V explained)

3. https://statquest.org/
    https://www.youtube.com/watch?v=KphmOJnLAdI (The matrix math behind transformer neural networks, one step at a time!!!)

4. https://ruslanmv.com/blog/Mathematics-of-a-Basic-Generative-Pretrained-Transformer
   https://medium.com/@kyeg/attention-mechanisms-simplified-using-einsum-in-pytorch-b99d3b4ef6f7

5. https://jalammar.github.io/illustrated-transformer/

6. Transformer Explainer
    https://poloclub.github.io/transformer-explainer/

Understanding Logits, Sigmoid, Softmax, and Cross-Entropy Loss in Deep Learning
===================================================================
https://wandb.ai/amanarora/Written-Reports/reports/Understanding-Logits-Sigmoid-Softmax-and-Cross-Entropy-Loss-in-Deep-Learning--Vmlldzo0NDMzNTU3

Deep Learning videos
==================
https://www.3blue1brown.com/topics/neural-networks
  https://www.youtube.com/@3blue1brown/videos
    https://www.youtube.com/watch?v=wjZofJX0v4M
    https://www.youtube.com/watch?v=eMlx5fFNoYc

The busy person's intro to LLMs
==========================
https://karpathy.ai/zero-to-hero.html
   https://www.youtube.com/watch?v=XfpMkf4rD6E&t=615s
   https://www.youtube.com/watch?v=zjkBMFhNj_g
   https://www.youtube.com/watch?v=kCc8FmEb1nY
   https://www.youtube.com/watch?v=zduSFxRajkE
   https://github.com/karpathy/nanoGPT/blob/master/model.py (nanoGPT)
https://levelup.gitconnected.com/lets-build-our-own-gpt-model-from-scratch-with-pytorch-236a65a1fb54

build, train and infer LLM
=====================
1. https://blog.miguelgrinberg.com/post/how-llms-work-explained-without-math
2. https://vickiboykis.com/2024/02/28/gguf-the-long-way-around/
3. https://ruslanmv.com/blog/How-to-Build-a-basic-LLM-GPT-from-Scratch-in-Python
4. https://www.omrimallis.com/posts/understanding-how-llm-inference-works-with-llama-cpp/
5. https://pub.towardsai.net/build-your-own-large-language-model-llm-from-scratch-using-pytorch-9e9945c24858
6. https://www.pluralsight.com/resources/blog/data/how-build-large-language-model
7. https://learningdeeplearning.com/post/why-gpus-are-faster-than-cpus/

DeepLearning.AI
==============
https://github.com/MalayAgr/generative-ai-with-llms-notes

AI Agents
=========
https://github.com/krohling/bondai
https://huggingface.co/blog/open-source-llms-as-agents
https://towardsdatascience.com/using-langchain-react-agents-for-answering-multi-hop-questions-in-rag-systems-893208c1847e
https://medium.com/data-science/using-langchain-react-agents-for-answering-multi-hop-questions-in-rag-systems-893208c1847e
  https://github.com/V-Sher/LangChain_ReAct_Demo/tree/main
  https://airbyte.com/data-engineering-resources/using-langchain-react-agents
https://confluence.oraclecorp.com/confluence/display/CCS/AI+Agent+-+Architecture
https://confluence.oraclecorp.com/confluence/display/IBS/Agent-Oriented+Architecture+for+CAA
https://confluence.oraclecorp.com/confluence/display/IBS/Agent-Oriented+Architecture+vs.+AI+Agentic+Patterns
https://confluence.oraclecorp.com/confluence/pages/viewpage.action?pageId=9930556902

https://atom-vb-idb6enfdcxbl.builder.us-chicago-1.ocp.oraclecloud.com/ic/builder/rt/ATOM_ODA/live/webApps/atom/
https://confluence.oraclecorp.com/confluence/pages/viewpage.action?pageId=10106151216

https://medium.com/data-science-collective/langgraph-mcp-ollama-the-key-to-powerful-agentic-ai-e1881f43cf63
  https://www.youtube.com/watch?v=Iy1yF_SAmUY&t=317s
https://medium.com/predict/using-the-model-context-protocol-mcp-with-a-local-llm-e398d6f318c3 (try)

https://medium.com/@anuragmishra_27746/building-multi-agents-supervisor-system-from-scratch-with-langgraph-langsmith-b602e8c2c95d
https://github.com/anurag-mishra899/Multi-Agents-Appointment-Booking
https://medium.com/data-science/agentic-ai-building-autonomous-systems-from-scratch-8f80b07229ea


OCI GenAI
=========
https://github.com/ou-developers/ou-generativeai-pro/tree/main/demos

. . .
Understanding and Coding Self-Attention, Multi-Head Attention, Cross-Attention, and Causal-Attention in LLMs
============================================================================================================
1. https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention
    https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder

Explain Q K V in attention
=========================
https://towardsdatascience.com/how-gpt-works-a-metaphoric-explanation-of-key-value-query-in-attention-using-a-tale-of-potion-8c66ace1f470

1. Q scores K
2. Softmax normalizes scores
3. Weight V by score

matrix vs tensor
================
https://medium.com/@quantumsteinke/whats-the-difference-between-a-matrix-and-a-tensor-4505fbdc576c#:~:text=A%20tensor%20is%20often%20thought,that%20is%20harder%20to%20visualize.

Run LLMs on Your CPU with Llama.cpp
===================================
$ pip install --force-reinstall --ignore-installed --no-cache-dir llama-cpp-python
1. https://awinml.github.io/llm-ggml-python/
2. https://github.com/ggerganov/llama.cpp/discussions/4531

llama.cpp performs the following steps:
It initializes a llama context from the gguf file using the llama_init_from_file function. This function reads the header and the body of the gguf file and creates a llama context object, which contains the model information and the backend to run the model on (CPU, GPU, or Metal).
It tokenizes the input text using the llama_tokenize function. This function converts the input text into a sequence of tokens based on the tokenizer specified in the gguf file header. The tokens are stored in an array of llama tokens, which are integers that represent the token IDs.
It generates the output text using the llama_generate function. This function takes the input tokens and the llama context as arguments and runs the model on the backend. It uses the computation graph specified in the gguf file header to perform the forward pass of the model and calculate the next token probabilities. It then samples the next token from the probability distribution and appends it to the output tokens. It repeats this process until the end-of-text token or the maximum number of tokens is reached. The output tokens are stored in another array of llama tokens.
It detokenizes the output text using the llama_detokenize function. This function converts the output tokens into a string of text based on the tokenizer specified in the gguf file header. It handles the special tokens, such as the end-of-text token, the padding token, and the unknown token, and returns the final output text.

Ollama local:
1. https://www.datacamp.com/tutorial/run-llama-3-locally

llama.cpp docker:
https://towardsdatascience.com/designing-building-deploying-an-ai-chat-app-from-scratch-part-1-f1ebf5232d4d
https://github.com/jsbaan/ai-app-from-scratch/blob/main/lm-api/README.md
https://medium.com/@mb20261/llm-by-examples-build-llama-cpp-with-customized-docker-images-4bb81ffcec2d


Oracel Vector Store
==================
https://luca-bindi.medium.com/oracle23ai-simplifies-rag-implementation-for-enterprise-llm-interaction-in-enterprise-solutions-d865dacdd1ed

https://github.com/ou-developers/ou-generativeai-pro/tree/main/demos

https://ryotayamanaka.medium.com/vector-search-in-oracle-23ai-cac685832d69

Tokenization
============
https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation

Run Oracle Database 23ai locally
================================
https://ronekins.com/2024/07/02/run-oracle-database-23ai-free-on-mac-computers-with-apple-silicon/

docker run -d \
  --name ora23ai \
  -p 1521:1521 \
  --mount source=oradata,target=/opt/oracle/oradata \
  container-registry.oracle.com/database/free

docker exec -it ora23ai ./setPassword.sh Welcome1
sql system/Welcome1@//localhost:1521/FREEPDB1

docker logs ora23ai
docker exec -it ora23ai ./setPassword.sh Welcome1
docker port ora23ai
sql system/Welcome1@//localhost:1521/FREE
sql system/Welcome1@//localhost:1521/FREEPDB1
docker exec -it ora23ai sqlplus / as sysdba

If you see below error:
error: ORA-43853: VECTOR type cannot be used in non-automatic segment space management tablespace "SYSTEM"

do as below:
SELECT tablespace_name,  segment_space_management FROM dba_tablespaces;

grant dba to testuser;
CREATE TABLESPACE tbs1 DATAFILE 'tbs1_data.dbf' SIZE 500m;
ALTER USER testuser QUOTA UNLIMITED ON tbs1
.
.
.
The amazing power of word vectors
https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/

https://jonathan-hui.medium.com/nlp-word-embedding-glove-5e7f523999f6
https://medium.com/@saschametzger/what-are-tokens-vectors-and-embeddings-how-do-you-create-them-e2a3e698e037

https://thenewstack.io/the-building-blocks-of-llms-vectors-tokens-and-embeddings/
https://www.linkedin.com/pulse/understanding-core-components-llms-vectors-tokens-embeddings-jain-dlv6e/


Udemy:
https://www.udemy.com/course/mathematics-behind-large-language-models-and-transformers/?referralCode=D45F8B15CE6BCC8A2159&couponCode=LEARNNOWPLANS

Build a Large Language Model (From Scratch), Video Edition
https://learning.oreilly.com/library/view/build-a-large/9781633437166/OEBPS/Text/chapter-3.html
https://www.udemy.com/course/mathematics-behind-large-language-models-and-transformers/?referralCode=D45F8B15CE6BCC8A2159&couponCode=LEARNNOWPLANS
https://ruslanmv.com/blog/AI-Medical-Chatbot
https://ruslanmv.com/blog/ArXiv-Research-with-Artificial-Intelligence

Quote:
Agents are tools meant to assist, and the role of human engineers will continue to evolve as AI takes on more responsibilities.

multi-agent:
`chat_model.bind_tools([tools]).invoke(messages)` will only make the LLM call with tool parameters bound, and return the raw AIMessage with tool_calls set (i.e. the parameters that a tool should be called with) if the model decided to call any of the tools (example doc: https://python.langchain.com/docs/how_to/tool_calling/#tool-calls )

`langgraph.prebuilt.create_react_agent(chat_model, tools=[tools]).invoke({"messages": messages})` will do the above, call the tool, pass the tool result (as a ToolMessage) back to the chat model, and keep executing that loop until the model only returns an AIMessage without any tool calls (doc: https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/#usage )

https://medium.com/@vipra_singh/ai-agents-build-an-agent-from-scratch-part-2-7ae11840c93a
https://medium.com/@anuragmishra_27746/hands-on-build-agentic-workflow-using-langgraph-key-learnings-from-langchain-academy-part-1-9ad96d6c9210
https://medium.com/@anuragmishra_27746/building-multi-agents-supervisor-system-from-scratch-with-langgraph-langsmith-b602e8c2c95d
  https://github.com/anurag-mishra899/Multi-Agents-Appointment-Booking

Local Inference with Meta’s Latest Llama 3.2 LLMs Using Ollama, LangChain, and Streamlit:
https://garystafford.medium.com/interacting-with-metas-latest-llama-3-2-models-using-ollama-langchain-and-streamlit-71f898b184d4

python:
https://pythonforthelab.com/blog/complete-guide-to-imports-in-python-absolute-relative-and-more/ (imports in python)
https://medium.com/everyday-ai/every-ai-engineer-should-know-these-15-python-libraries-in-2025-b83726ecc86d

MCP:
Using the Model Context Protocol (MCP) With a Local LLM
Any large language model that supports function calling (or tool use) is capable of making use of the model context protocol. In this tutorial article, we’ll start by creating an MCP server that offers a few tools, and then get a small Llama 3.2 model to make use of those tools.
MCP is a relatively new protocol, introduced by Anthropic, that aims to simplify and standardize how large language models (LLMs) interact with external data. These days, it’s also the hottest topic on X and several AI-related forums.
We need to think of MCP as a universal adapter that lets LLMs access real-world data (like files, databases, APIs, and services) in a consistent, scalable way.
https://medium.com/predict/using-the-model-context-protocol-mcp-with-a-local-llm-e398d6f318c3

The base MCP protocol involves creating, sending, and receiving JSON-RPC messages. So, let’s create some utility functions for them.
According to the protocol, the first message we need to send is an initialization message, which starts our client’s conversation with the MCP server. In this message the client just introduces itself to the server, specifying its name and version. The name of the MCP method for this step is initialize. The server will reply introducing itself, giving various details about its capabilities. And to say that the initialization is complete, we need to call a method named notifications/initialized. This doesn’t need any parameters, and the server won’t reply back this time.

<|python_tag|> and <|eom_id|>

So we need to extract the JSON string between those tags and convert it into a JSON-RPC message we can send to the MCP server.

Knowledge Graph Pipeline:
https://levelup.gitconnected.com/converting-unstructured-data-into-a-knowledge-graph-using-an-end-to-end-pipeline-552a508045f9
  https://github.com/FareedKhan-dev/KG-Pipeline

Multi-Agents:
https://medium.com/@anuragmishra_27746/building-multi-agents-supervisor-system-from-scratch-with-langgraph-langsmith-b602e8c2c95d
  https://github.com/anurag-mishra899/Multi-Agents-Appointment-Booking

AI Agents & Automation:
   ollama:
      pull ollama model: ollama pull llama3.2
      run ollama model : ollama run llama3.2
      The default connection URL for Ollama is typically http://localhost:11434.
   n8n:
      docker volume create n8n_data
      docker run -it --rm --name n8n -p 5678:5678 -v n8n_data:/home/node/.n8n docker.n8n.io/n8nio/n8n
      to access ollama from within the n8n cluster, the connect url: http://host.docker.internal:11434

